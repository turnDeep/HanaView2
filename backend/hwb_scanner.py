import json
import os
import asyncio
import concurrent.futures
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
import pandas as pd
import numpy as np
import uuid
from dotenv import load_dotenv
from .hwb_data_manager import HWBDataManager
import logging
import warnings

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)
load_dotenv()

# --- Constants ---
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '50'))
MAX_WORKERS = int(os.getenv('MAX_WORKERS', '10'))

# Rule 1: Trend Filter
WEEKLY_TREND_THRESHOLD = float(os.getenv('WEEKLY_TREND_THRESHOLD', '0.0'))

# Rule 2: Setup
SETUP_LOOKBACK_DAYS = int(os.getenv('SETUP_LOOKBACK_DAYS', '30'))

# Rule 3: FVG Detection
FVG_MIN_GAP_PERCENTAGE = float(os.getenv('FVG_MIN_GAP_PERCENTAGE', '0.001'))
FVG_MAX_SEARCH_DAYS = int(os.getenv('FVG_MAX_SEARCH_DAYS', '20'))
MIN_FVG_SCORE = int(os.getenv('MIN_FVG_SCORE', '3'))

# Rule 4: Breakout
MIN_BREAKOUT_SCORE = int(os.getenv('MIN_BREAKOUT_SCORE', '5'))


class HWBAnalyzer:
    """HWBåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆçŠ¶æ…‹ç®¡ç†æ”¹å–„ç‰ˆï¼‰"""
    
    def __init__(self):
        self.market_regime = 'TRENDING'
        self.params = self._adaptive_parameters()

    def _adaptive_parameters(self):
        """å¸‚å ´ç’°å¢ƒã«å¿œã˜ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´"""
        params = {
            'TRENDING': {
                'setup_lookback': 30, 
                'fvg_search_days': 20, 
                'ma_proximity': 0.05, 
                'breakout_threshold': 0.003
            },
        }
        return params.get(self.market_regime, params['TRENDING'])

    def optimized_rule1(self, df_daily: pd.DataFrame, df_weekly: pd.DataFrame) -> bool:
        """Rule â‘ : é€±è¶³ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"""
        if df_weekly is None or df_weekly.empty:
            return False
        if 'sma200' not in df_weekly.columns or df_weekly['sma200'].isna().all():
            return False

        latest_weekly = df_weekly.iloc[-1]
        if pd.isna(latest_weekly['sma200']) or latest_weekly['sma200'] == 0:
            return False

        weekly_deviation = (latest_weekly['close'] - latest_weekly['sma200']) / latest_weekly['sma200']
        return weekly_deviation >= WEEKLY_TREND_THRESHOLD

    def optimized_rule2_setups(self, df_daily: pd.DataFrame) -> List[Dict]:
        """Rule â‘¡: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¤œå‡ºï¼ˆIDä»˜ä¸ç‰ˆï¼‰"""
        setups = []
        lookback_days = self.params['setup_lookback']
        if len(df_daily) < lookback_days:
            return setups

        # ATRè¨ˆç®—
        atr = (df_daily['high'] - df_daily['low']).rolling(14).mean()

        scan_start_index = max(0, len(df_daily) - lookback_days)
        
        for i in range(scan_start_index, len(df_daily)):
            row = df_daily.iloc[i]
            if pd.isna(row.get('sma200')) or pd.isna(row.get('ema200')):
                continue

            # MAã‚¾ãƒ¼ãƒ³è¨ˆç®—
            zone_width = abs(row['sma200'] - row['ema200'])
            if atr.iloc[i] > 0:
                zone_width = max(zone_width, row['close'] * (atr.iloc[i] / row['close']) * 0.5)

            zone_upper = max(row['sma200'], row['ema200']) + zone_width * 0.2
            zone_lower = min(row['sma200'], row['ema200']) - zone_width * 0.2

            # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—åˆ¤å®š
            if zone_lower <= row['open'] <= zone_upper and zone_lower <= row['close'] <= zone_upper:
                setup = {
                    'id': str(uuid.uuid4()),  # ãƒ¦ãƒ‹ãƒ¼ã‚¯IDä»˜ä¸
                    'date': df_daily.index[i],
                    'type': 'PRIMARY',
                    'confidence': 0.85,
                    'status': 'active'  # åˆæœŸçŠ¶æ…‹
                }
                setups.append(setup)
            elif (zone_lower <= row['open'] <= zone_upper) or (zone_lower <= row['close'] <= zone_upper):
                body_center = (row['open'] + row['close']) / 2
                if zone_lower <= body_center <= zone_upper:
                    setup = {
                        'id': str(uuid.uuid4()),
                        'date': df_daily.index[i],
                        'type': 'SECONDARY',
                        'confidence': 0.65,
                        'status': 'active'
                    }
                    setups.append(setup)
        
        return setups

    def optimized_fvg_detection(self, df_daily: pd.DataFrame, setup: Dict) -> List[Dict]:
        """Rule â‘¢: FVGæ¤œå‡ºï¼ˆsetup_idç´ä»˜ã‘ç‰ˆï¼‰"""
        fvgs = []
        setup_date = setup['date']
        
        try:
            setup_idx = df_daily.index.get_loc(setup_date)
        except KeyError:
            return fvgs

        max_days = self.params['fvg_search_days']
        search_end = min(setup_idx + max_days, len(df_daily) - 1)

        vol_rolling_mean = df_daily['volume'].rolling(20).mean()
        volatility = df_daily['close'].pct_change().rolling(20).std()

        for i in range(setup_idx + 2, search_end):
            candle_1, candle_3 = df_daily.iloc[i-2], df_daily.iloc[i]
            
            # FVGæ¡ä»¶: candle_3ã®lowãŒcandle_1ã®highã‚ˆã‚Šä¸Š
            if candle_3['low'] <= candle_1['high']:
                continue

            gap_percentage = (candle_3['low'] - candle_1['high']) / candle_1['high']
            if gap_percentage < FVG_MIN_GAP_PERCENTAGE:
                continue

            # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            fvg_score = 0
            if gap_percentage > 0.005:
                fvg_score += 3
            elif gap_percentage > 0.002:
                fvg_score += 2
            else:
                fvg_score += 1

            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç¢ºèª
            volume_surge = candle_3['volume'] / vol_rolling_mean.iloc[i] if vol_rolling_mean.iloc[i] > 0 else 1
            if volume_surge > 1.5:
                fvg_score += 2
            elif volume_surge > 1.2:
                fvg_score += 1

            # MA proximity
            ma_deviation = None
            ma_center = (candle_3.get('sma200', 0) + candle_3.get('ema200', 0)) / 2
            if ma_center > 0:
                price_center = (candle_3['open'] + candle_3['close']) / 2
                ma_deviation = abs(price_center - ma_center) / ma_center
                current_volatility = volatility.iloc[i]
                if pd.notna(current_volatility):
                    dynamic_threshold = min(0.05 + current_volatility * 2, 0.10)
                    if ma_deviation <= dynamic_threshold * 0.5:
                        fvg_score += 3
                    elif ma_deviation <= dynamic_threshold:
                        fvg_score += 2
                    elif ma_deviation <= dynamic_threshold * 1.5:
                        fvg_score += 1

            if fvg_score >= MIN_FVG_SCORE:
                fvg = {
                    'id': str(uuid.uuid4()),  # FVGã«ã‚‚IDä»˜ä¸
                    'setup_id': setup['id'],  # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ç´ä»˜ã‘
                    'formation_date': df_daily.index[i],
                    'gap_percentage': gap_percentage,
                    'score': fvg_score,
                    'volume_surge': volume_surge,
                    'ma_deviation': ma_deviation,
                    'quality': 'HIGH' if fvg_score >= 6 else 'MEDIUM' if fvg_score >= 4 else 'LOW',
                    'lower_bound': candle_1['high'],
                    'upper_bound': candle_3['low'],
                    'status': 'active'  # åˆæœŸçŠ¶æ…‹
                }
                fvgs.append(fvg)
        
        return sorted(fvgs, key=lambda x: x['score'], reverse=True)

    def optimized_breakout_detection_all_periods(
        self, 
        df_daily: pd.DataFrame, 
        setup: Dict, 
        fvg: Dict
    ) -> Optional[Dict]:
        """
        Rule â‘£: ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ¤œå‡ºï¼ˆå…¨æœŸé–“ã‚¹ã‚­ãƒ£ãƒ³ç‰ˆï¼‰
        
        é‡è¦ãªå¤‰æ›´ç‚¹:
        - FVGå½¢æˆæ—¥ã‹ã‚‰ç¾åœ¨ã¾ã§ã€Œå…¨ã¦ã®æ—¥ã€ã‚’ãƒã‚§ãƒƒã‚¯
        - æœ€åˆã«ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã—ãŸæ—¥ã‚’æ¤œå‡º
        """
        try:
            setup_idx = df_daily.index.get_loc(setup['date'])
            fvg_idx = df_daily.index.get_loc(fvg['formation_date'])
        except KeyError:
            return None

        # ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«è¨ˆç®—ï¼ˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€œFVGå½¢æˆã¾ã§ï¼‰
        lookback_window = min(20, fvg_idx - setup_idx)
        resistance_data = df_daily.iloc[max(0, fvg_idx - lookback_window) : fvg_idx]
        
        if resistance_data.empty:
            return None

        resistance_levels = {
            'high': resistance_data['high'].max(),
            'close': resistance_data['close'].max(),
            'vwap': (resistance_data['close'] * resistance_data['volume']).sum() / resistance_data['volume'].sum() 
                    if resistance_data['volume'].sum() > 0 
                    else resistance_data['close'].mean(),
            'pivot': (resistance_data['high'].max() + resistance_data['low'].min() + resistance_data['close'].iloc[-1]) / 3
        }
        main_resistance = np.median(list(resistance_levels.values()))

        # FVGé•åãƒã‚§ãƒƒã‚¯ï¼ˆFVGå½¢æˆå¾Œã«ä¸‹é™ã‚’å¤§ããå‰²ã‚Šè¾¼ã‚“ã ã‚‰ç„¡åŠ¹ï¼‰
        post_fvg_data = df_daily.iloc[fvg_idx:]
        if post_fvg_data['low'].min() < fvg['lower_bound'] * 0.98:
            return {'status': 'violated', 'violated_date': post_fvg_data['low'].idxmin()}

        # ğŸ”¥ é‡è¦: FVGå½¢æˆæ—¥ã‹ã‚‰ç¾åœ¨ã¾ã§ã€å„æ—¥ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
        vol_ma = df_daily['volume'].rolling(20).mean()
        
        for i in range(fvg_idx + 1, len(df_daily)):
            current = df_daily.iloc[i]
            recent_volatility = df_daily['close'].pct_change().rolling(20).std().iloc[i]
            breakout_threshold = max(self.params['breakout_threshold'], min(0.01, recent_volatility * 3))

            # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ¡ä»¶
            cond_price = current['close'] > main_resistance * (1 + breakout_threshold)
            cond_volume = current['volume'] > vol_ma.iloc[i] * 1.2 if pd.notna(vol_ma.iloc[i]) else False
            cond_momentum = df_daily['close'].pct_change(5).iloc[i] > 0

            breakout_score = sum([
                3 if cond_price else 0,
                2 if cond_volume else 0,
                1 if cond_momentum else 0
            ])

            if breakout_score >= MIN_BREAKOUT_SCORE:
                return {
                    'status': 'breakout',
                    'breakout_date': df_daily.index[i],  # ã“ã®æ—¥ã«ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ
                    'breakout_price': current['close'],
                    'resistance_price': main_resistance,
                    'breakout_score': breakout_score,
                    'confidence': 'HIGH' if breakout_score >= 7 else 'MEDIUM',
                }
        
        # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆãªã—
        return None


class HWBScanner:
    """ãƒ¡ã‚¤ãƒ³ã‚¹ã‚­ãƒ£ãƒŠãƒ¼ï¼ˆçŠ¶æ…‹ç®¡ç†ç‰ˆï¼‰"""
    
    def __init__(self):
        self.data_manager = HWBDataManager()
        self.analyzer = HWBAnalyzer()

    async def scan_all_symbols(self, progress_callback=None):
        """å…¨ã‚·ãƒ³ãƒœãƒ«ã‚¹ã‚­ãƒ£ãƒ³"""
        symbols = list(self.data_manager.get_russell3000_symbols())
        total = len(symbols)
        logger.info(f"ã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹: {total}éŠ˜æŸ„")
        scan_start_time = datetime.now()

        all_results = []
        processed_count = 0
        
        for i in range(0, total, BATCH_SIZE):
            batch = symbols[i:i + BATCH_SIZE]
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_symbol = {
                    executor.submit(self._analyze_and_save_symbol, symbol): symbol 
                    for symbol in batch
                }
                for future in concurrent.futures.as_completed(future_to_symbol):
                    processed_count += 1
                    try:
                        result = future.result()
                        if result:
                            all_results.extend(result)
                    except Exception as exc:
                        logger.error(f"ã‚¨ãƒ©ãƒ¼: {future_to_symbol[future]} - {exc}", exc_info=True)
                    if progress_callback:
                        await progress_callback(processed_count, total)
            await asyncio.sleep(0.1)

        summary = self._create_daily_summary(all_results, total, scan_start_time)
        self.data_manager.save_daily_summary(summary)
        logger.info("ã‚¹ã‚­ãƒ£ãƒ³å®Œäº†")
        return summary

    def _analyze_and_save_symbol(self, symbol: str) -> Optional[List[Dict]]:
        """
        å˜ä¸€éŠ˜æŸ„åˆ†æï¼ˆçŠ¶æ…‹ãƒ™ãƒ¼ã‚¹ã®å·®åˆ†å‡¦ç†ç‰ˆï¼‰
        
        å‡¦ç†ãƒ•ãƒ­ãƒ¼:
        1. æ—¢å­˜ã®JSONã‚’èª­ã¿è¾¼ã¿ã€çŠ¶æ…‹ã‚’ç¢ºèª
        2. çŠ¶æ…‹ã«å¿œã˜ã¦å¿…è¦ãªå‡¦ç†ã®ã¿å®Ÿè¡Œ
           - active setup â†’ FVGæ¤œå‡ºï¼ˆæ–°ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
           - active FVG â†’ ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ¤œå‡ºï¼ˆæ–°ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
           - consumed â†’ æ–°ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¤œå‡ºï¼ˆæœ€æ–°æ—¥ã®ã¿ï¼‰
        """
        try:
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾—
            data = self.data_manager.get_stock_data_with_cache(symbol)
            if not data:
                return None
            
            df_daily, df_weekly = data
            if df_daily.empty or df_weekly.empty:
                return None

            df_daily.index = pd.to_datetime(df_daily.index)
            df_weekly.index = pd.to_datetime(df_weekly.index)
            df_daily = df_daily[~df_daily.index.duplicated(keep='last')]
            df_weekly = df_weekly[~df_weekly.index.duplicated(keep='last')]

            # Rule â‘ : ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if not self.analyzer.optimized_rule1(df_daily, df_weekly):
                return None

            # æ—¢å­˜ã®JSONèª­ã¿è¾¼ã¿ï¼ˆçŠ¶æ…‹ç¢ºèªï¼‰
            existing_data = self.data_manager.load_symbol_data(symbol)
            
            if existing_data:
                # ğŸ“„ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š â†’ å·®åˆ†å‡¦ç†
                result = self._differential_analysis(
                    symbol, df_daily, df_weekly, existing_data
                )
            else:
                # ğŸ†• æ–°è¦éŠ˜æŸ„ â†’ ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³
                result = self._full_analysis(
                    symbol, df_daily, df_weekly
                )
            
            return result

        except Exception as e:
            logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {symbol} - {e}", exc_info=True)
            return None

    def _differential_analysis(
        self, 
        symbol: str, 
        df_daily: pd.DataFrame, 
        df_weekly: pd.DataFrame,
        existing_data: dict
    ) -> Optional[List[Dict]]:
        """
        å·®åˆ†åˆ†æ: æ—¢å­˜ã®çŠ¶æ…‹ã«åŸºã¥ã„ã¦å¿…è¦ãªå‡¦ç†ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        
        é‡è¦ãªå¤‰æ›´:
        - FVGã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšã€activeãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯ç¶™ç¶šçš„ã«FVGã‚’æ¢ã™
        - è¤‡æ•°ã®FVGãŒåŒã˜ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‹ã‚‰ç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’è¨±å®¹
        """
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çŠ¶æ…‹ã‚’å–å¾—
        existing_setups = existing_data.get('setups', [])
        existing_fvgs = existing_data.get('fvgs', [])
        existing_signals = existing_data.get('signals', [])
        
        # æ—¥ä»˜å¤‰æ›
        for item in existing_setups + existing_fvgs + existing_signals:
            if 'date' in item:
                item['date'] = pd.to_datetime(item['date'])
            if 'formation_date' in item:
                item['formation_date'] = pd.to_datetime(item['formation_date'])
            if 'breakout_date' in item:
                item['breakout_date'] = pd.to_datetime(item['breakout_date'])
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªçŠ¶æ…‹ã‚’ç¢ºèª
        active_setups = [s for s in existing_setups if s.get('status') == 'active']
        active_fvgs = [f for f in existing_fvgs if f.get('status') == 'active']
        
        # æœ€æ–°ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ã‚’å–å¾—
        last_analyzed_date = pd.to_datetime(existing_data.get('last_updated', '2000-01-01')).date()
        latest_data_date = df_daily.index[-1].date()
        
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯çµ‚äº†
        if latest_data_date <= last_analyzed_date:
            logger.debug(f"{symbol}: æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãªã—")
            return self._create_summary_from_existing(existing_data)
        
        logger.info(f"{symbol}: å·®åˆ†åˆ†æ ({last_analyzed_date} â†’ {latest_data_date})")
        
        updated = False
        new_fvgs_found = []
        
        # ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‹ã‚‰æ–°ã—ã„FVGã‚’æ¢ã™
        if active_setups:
            logger.info(f"{symbol}: FVGæ¢ç´¢ï¼ˆ{len(active_setups)}ä»¶ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼‰")
            
            for setup in active_setups:
                setup_date = setup['date']
                setup_idx = df_daily.index.get_loc(setup_date)
                
                # ã“ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«ç´ã¥ãæ—¢å­˜FVGã®æœ€å¾Œã®æ—¥ä»˜ã‚’å–å¾—
                setup_fvgs = [f for f in existing_fvgs if f.get('setup_id') == setup['id']]
                if setup_fvgs:
                    # æ—¢ã«FVGãŒã‚ã‚‹å ´åˆã€æœ€å¾Œã®FVGå½¢æˆæ—¥ã®æ¬¡ã®æ—¥ã‹ã‚‰æ¢ç´¢
                    last_fvg_date = max(f['formation_date'] for f in setup_fvgs)
                    search_start_date = last_fvg_date + pd.Timedelta(days=1)
                    search_start = df_daily.index.searchsorted(search_start_date)
                else:
                    # FVGãŒã¾ã ãªã„å ´åˆã€ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—+2æ—¥ç›®ã‹ã‚‰
                    search_start = setup_idx + 2
                
                # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‹ã‚‰æœ€å¤§20æ—¥å…ˆã¾ã§
                search_end = min(setup_idx + FVG_MAX_SEARCH_DAYS, len(df_daily) - 1)
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²ã«é™å®š
                new_data_start = df_daily.index.searchsorted(
                    pd.Timestamp(last_analyzed_date) + pd.Timedelta(days=1)
                )
                search_start = max(search_start, new_data_start)
                
                if search_start >= search_end:
                    continue
                
                # æ–°ã—ã„ç¯„å›²ã§FVGæ¤œå‡º
                new_fvgs = self._detect_fvg_in_range(
                    df_daily, setup, search_start, search_end
                )
                
                if new_fvgs:
                    logger.info(f"{symbol}: {len(new_fvgs)}ä»¶ã®æ–°FVGæ¤œå‡ºï¼ˆsetup: {setup['id'][:8]}...ï¼‰")
                    existing_data['fvgs'].extend(new_fvgs)
                    new_fvgs_found.extend(new_fvgs)
                    updated = True
        
        # ğŸ¯ ã‚¹ãƒ†ãƒƒãƒ—2: å…¨ã¦ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–FVGï¼ˆæ—¢å­˜+æ–°è¦ï¼‰ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
        all_active_fvgs = active_fvgs + new_fvgs_found
        
        if all_active_fvgs:
            logger.info(f"{symbol}: ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆ{len(all_active_fvgs)}ä»¶ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–FVGï¼‰")
            
            for fvg in all_active_fvgs:
                # å¯¾å¿œã™ã‚‹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’å–å¾—
                setup = next((s for s in existing_setups if s['id'] == fvg['setup_id']), None)
                if not setup:
                    continue
                
                # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒæ—¢ã«æ¶ˆè²»æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                if setup.get('status') == 'consumed':
                    continue
                
                # FVGå½¢æˆæ—¥ä»¥é™ã®æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒã‚§ãƒƒã‚¯
                fvg_date = fvg['formation_date']
                fvg_idx = df_daily.index.get_loc(fvg_date)
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®é–‹å§‹ä½ç½®
                new_data_start = df_daily.index.searchsorted(
                    pd.Timestamp(last_analyzed_date) + pd.Timedelta(days=1)
                )
                check_start = max(fvg_idx + 1, new_data_start)
                
                if check_start >= len(df_daily):
                    continue
                
                # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
                breakout = self._check_breakout_in_range(
                    df_daily, setup, fvg, check_start, len(df_daily)
                )
                
                if breakout and breakout.get('status') == 'breakout':
                    # ğŸ¯ ã‚·ã‚°ãƒŠãƒ«ç™ºç”Ÿï¼
                    signal = {**fvg, **breakout}
                    signal['score'] = self._calculate_signal_score(signal)
                    existing_data['signals'].append(signal)
                    
                    # ğŸ”¥ é‡è¦: ã“ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«é–¢é€£ã™ã‚‹å…¨ã¦ã‚’æ¶ˆè²»
                    setup['status'] = 'consumed'
                    
                    # åŒã˜setup_idã‚’æŒã¤å…¨ã¦ã®FVGã‚’æ¶ˆè²»
                    consumed_count = 0
                    for related_fvg in existing_data['fvgs']:
                        if related_fvg.get('setup_id') == setup['id']:
                            related_fvg['status'] = 'consumed'
                            consumed_count += 1
                    
                    updated = True
                    
                    logger.info(
                        f"{symbol}: ã‚·ã‚°ãƒŠãƒ«ç™ºç”Ÿ @ {breakout['breakout_date']} "
                        f"(setup: {setup['id'][:8]}..., FVGæ¶ˆè²»æ•°: {consumed_count})"
                    )
                    
                    # âœ… 1ã¤ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‹ã‚‰1ã¤ã®ã‚·ã‚°ãƒŠãƒ«ã®ã¿
                    # ã“ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«ç´ã¥ãä»–ã®FVGã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    break
                
                elif breakout and breakout.get('status') == 'violated':
                    # FVGé•å
                    fvg['status'] = 'violated'
                    fvg['violated_date'] = breakout.get('violated_date')
                    updated = True
        
        # ğŸ†• ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚‚FVGã‚‚ãªã„å ´åˆã€æ–°ã—ã„ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’æ¢ã™
        if not active_setups or all(s.get('status') == 'consumed' for s in existing_setups):
            logger.info(f"{symbol}: æ–°ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¢ç´¢")
            
            # æœ€æ–°ã®æ•°æ—¥ã®ã¿ãƒã‚§ãƒƒã‚¯
            check_days = 5
            recent_data = df_daily.tail(check_days)
            
            for i in range(len(recent_data)):
                row = recent_data.iloc[i]
                date = recent_data.index[i]
                
                if date.date() <= last_analyzed_date:
                    continue
                
                if pd.isna(row.get('sma200')) or pd.isna(row.get('ema200')):
                    continue
                
                # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—åˆ¤å®š
                zone_width = abs(row['sma200'] - row['ema200'])
                zone_upper = max(row['sma200'], row['ema200']) + zone_width * 0.2
                zone_lower = min(row['sma200'], row['ema200']) - zone_width * 0.2
                
                if zone_lower <= row['open'] <= zone_upper and zone_lower <= row['close'] <= zone_upper:
                    new_setup = {
                        'id': str(uuid.uuid4()),
                        'date': date,
                        'type': 'PRIMARY',
                        'confidence': 0.85,
                        'status': 'active'
                    }
                    existing_data['setups'].append(new_setup)
                    updated = True
                    logger.info(f"{symbol}: æ–°ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— @ {date.date()}")
                    break  # 1ã¤è¦‹ã¤ã‹ã£ãŸã‚‰çµ‚äº†
        
        # ğŸ“ ä¿å­˜
        if updated:
            existing_data['last_updated'] = datetime.now().isoformat()
            self._save_symbol_data_with_chart(symbol, existing_data, df_daily, df_weekly)
        
        return self._create_summary_from_existing(existing_data)

    def _full_analysis(
        self,
        symbol: str,
        df_daily: pd.DataFrame,
        df_weekly: pd.DataFrame
    ) -> Optional[List[Dict]]:
        """æ–°è¦éŠ˜æŸ„ã®ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆå¾“æ¥ã®å‡¦ç†ï¼‰"""
        logger.info(f"{symbol}: ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆæ–°è¦ï¼‰")
        
        # Rule â‘¡: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¤œå‡º
        setups = self.analyzer.optimized_rule2_setups(df_daily)
        if not setups:
            return None

        consumed_setups = set()
        consumed_fvgs = set()
        all_fvgs = []
        all_signals = []

        for s in setups:
            s['date'] = pd.to_datetime(s['date'])

        for setup in setups:
            if setup['id'] in consumed_setups:
                setup['status'] = 'consumed'
                continue

            fvgs = self.analyzer.optimized_fvg_detection(df_daily, setup)
            signal_found_for_this_setup = False
            
            for fvg in fvgs:
                if fvg['id'] in consumed_fvgs:
                    fvg['status'] = 'consumed'
                    all_fvgs.append(fvg)
                    continue

                breakout = self.analyzer.optimized_breakout_detection_all_periods(
                    df_daily, setup, fvg
                )

                if breakout:
                    if breakout.get('status') == 'breakout':
                        signal = {**fvg, **breakout}
                        signal['score'] = self._calculate_signal_score(signal)
                        all_signals.append(signal)
                        
                        consumed_setups.add(setup['id'])
                        consumed_fvgs.add(fvg['id'])
                        fvg['status'] = 'consumed'
                        setup['status'] = 'consumed'
                        signal_found_for_this_setup = True
                        break
                    
                    elif breakout.get('status') == 'violated':
                        fvg['status'] = 'violated'
                        fvg['violated_date'] = breakout.get('violated_date')
                else:
                    fvg['status'] = 'active'
                
                all_fvgs.append(fvg)
            
            if not signal_found_for_this_setup:
                setup['status'] = 'active'

        if not all_signals and not any(f['status'] == 'active' for f in all_fvgs):
            return None

        def stringify_dates(d):
            for k, v in d.items():
                if isinstance(v, pd.Timestamp):
                    d[k] = v.strftime('%Y-%m-%d')
            return d

        symbol_data = {
            "symbol": symbol,
            "last_updated": datetime.now().isoformat(),
            "market_regime": self.analyzer.market_regime,
            "setups": [stringify_dates(s.copy()) for s in setups],
            "fvgs": [stringify_dates(f.copy()) for f in all_fvgs],
            "signals": [stringify_dates(s.copy()) for s in all_signals]
        }
        
        self._save_symbol_data_with_chart(symbol, symbol_data, df_daily, df_weekly)
        return self._create_summary_from_data(symbol, all_signals, all_fvgs)

    def _detect_fvg_in_range(self, df_daily: pd.DataFrame, setup: Dict, start_idx: int, end_idx: int) -> List[Dict]:
        """æŒ‡å®šç¯„å›²å†…ã§FVGæ¤œå‡º"""
        fvgs = []
        vol_rolling_mean = df_daily['volume'].rolling(20).mean()
        
        for i in range(start_idx, end_idx):
            if i < 2:
                continue
            
            candle_1, candle_3 = df_daily.iloc[i-2], df_daily.iloc[i]
            if candle_3['low'] <= candle_1['high']:
                continue
            
            gap_percentage = (candle_3['low'] - candle_1['high']) / candle_1['high']
            if gap_percentage < FVG_MIN_GAP_PERCENTAGE:
                continue
            
            fvg_score = 1 if gap_percentage > 0.001 else 0
            if fvg_score >= MIN_FVG_SCORE:
                fvg = {
                    'id': str(uuid.uuid4()),
                    'setup_id': setup['id'],
                    'formation_date': df_daily.index[i],
                    'gap_percentage': gap_percentage,
                    'score': fvg_score,
                    'quality': 'MEDIUM',
                    'lower_bound': candle_1['high'],
                    'upper_bound': candle_3['low'],
                    'status': 'active'
                }
                fvgs.append(fvg)
        
        return fvgs

    def _check_breakout_in_range(self, df_daily: pd.DataFrame, setup: Dict, fvg: Dict, start_idx: int, end_idx: int) -> Optional[Dict]:
        """æŒ‡å®šç¯„å›²å†…ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯"""
        try:
            setup_idx = df_daily.index.get_loc(setup['date'])
            fvg_idx = df_daily.index.get_loc(fvg['formation_date'])
        except KeyError:
            return None
        
        lookback_window = min(20, fvg_idx - setup_idx)
        resistance_data = df_daily.iloc[max(0, fvg_idx - lookback_window) : fvg_idx]
        
        if resistance_data.empty:
            return None
        
        main_resistance = resistance_data['high'].max()
        
        for i in range(start_idx, end_idx):
            current = df_daily.iloc[i]
            if current['close'] > main_resistance * 1.003:
                return {
                    'status': 'breakout',
                    'breakout_date': df_daily.index[i],
                    'breakout_price': current['close'],
                    'resistance_price': main_resistance,
                    'breakout_score': 6,
                    'confidence': 'MEDIUM'
                }
        
        return None

    def _save_symbol_data_with_chart(self, symbol: str, symbol_data: dict, df_daily: pd.DataFrame, df_weekly: pd.DataFrame):
        """æ—¥ä»˜å¤‰æ›ã¨ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ä»˜ãã§ä¿å­˜"""
        def stringify_dates(d):
            for k, v in d.items():
                if isinstance(v, pd.Timestamp):
                    d[k] = v.strftime('%Y-%m-%d')
            return d
        
        symbol_data['setups'] = [stringify_dates(s.copy()) for s in symbol_data['setups']]
        symbol_data['fvgs'] = [stringify_dates(f.copy()) for f in symbol_data['fvgs']]
        symbol_data['signals'] = [stringify_dates(s.copy()) for s in symbol_data['signals']]
        
        symbol_data['chart_data'] = self._generate_lightweight_chart_data(symbol_data, df_daily, df_weekly)
        self.data_manager.save_symbol_data(symbol, symbol_data)

    def _create_summary_from_existing(self, existing_data: dict) -> List[Dict]:
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        symbol = existing_data['symbol']
        signals = existing_data.get('signals', [])
        fvgs = existing_data.get('fvgs', [])
        
        return self._create_summary_from_data(symbol, signals, fvgs)

    def _create_summary_from_data(self, symbol: str, signals: list, fvgs: list) -> List[Dict]:
        """ã‚·ã‚°ãƒŠãƒ«ã¨FVGã‹ã‚‰ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        summary_results = []
        
        for signal in signals:
            summary_results.append({
                "symbol": symbol,
                "signal_type": "signal",
                "score": signal.get('score', 50),
                "signal_date": signal.get('breakout_date')
            })
        
        for fvg in fvgs:
            if fvg.get('status') == 'active' and fvg.get('quality') in ['HIGH', 'MEDIUM']:
                summary_results.append({
                    "symbol": symbol,
                    "signal_type": "candidate",
                    "score": self._calculate_signal_score(fvg),
                    "fvg_date": fvg.get('formation_date')
                })
        
        return summary_results

    def _calculate_signal_score(self, signal_data: Dict) -> int:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        setup_score = signal_data.get('setup_confidence', 0.5) * 35
        fvg_score = (signal_data.get('score', 0) / 10) * 40
        breakout_score = (signal_data.get('breakout_score', 0) / 8) * 30 if 'breakout_score' in signal_data else 0
        return int(min(setup_score + fvg_score + breakout_score, 100))

    def _generate_lightweight_chart_data(self, symbol_data: dict, df_daily: pd.DataFrame, df_weekly: pd.DataFrame) -> dict:
        """ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        df_plot = df_daily.copy()
        df_plot['weekly_sma200_val'] = df_weekly['sma200'].reindex(df_plot.index, method='ffill')

        def format_series(df, col):
            s = df[[col]].dropna()
            return [{"time": i.strftime('%Y-%m-%d'), "value": r[col]} for i, r in s.iterrows()]

        def clean_np_types(d):
            for k, v in d.items():
                if isinstance(v, (np.int64, np.int32)):
                    d[k] = int(v)
                if isinstance(v, (np.float64, np.float32)):
                    d[k] = float(v)
            return d

        candles = [{
            "time": i.strftime('%Y-%m-%d'),
            "open": r.open,
            "high": r.high,
            "low": r.low,
            "close": r.close
        } for i, r in df_plot.iterrows()]

        # å‡ºæ¥é«˜ãƒ‡ãƒ¼ã‚¿
        volume_data = []
        for i, r in df_plot.iterrows():
            color = '#26a69a' if r['close'] >= r['open'] else '#ef5350'
            volume_data.append({
                "time": i.strftime('%Y-%m-%d'),
                "value": r['volume'],
                "color": color
            })

        # ãƒãƒ¼ã‚«ãƒ¼: FVGã¯ğŸ®ï¼ˆçœŸã‚“ä¸­ã®ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®ä¸Šã«ï¼‰
        markers = []

        # FVGãƒãƒ¼ã‚«ãƒ¼ï¼ˆçœŸã‚“ä¸­ã®ãƒ­ãƒ¼ã‚½ã‚¯è¶³ = formation_date ã®1ã¤å‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
        for fvg in symbol_data.get('fvgs', []):
            try:
                formation_date = pd.to_datetime(fvg['formation_date'])

                # formation_dateã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                if formation_date in df_plot.index:
                    formation_idx = df_plot.index.get_loc(formation_date)

                    # çœŸã‚“ä¸­ã®ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã¯1ã¤å‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                    if formation_idx >= 1:
                        middle_candle_date = df_plot.index[formation_idx - 1]

                        color_map = {
                            'active': '#FFD700',      # ã‚´ãƒ¼ãƒ«ãƒ‰
                            'consumed': '#9370DB',    # ç´«
                            'violated': '#808080'     # ã‚°ãƒ¬ãƒ¼
                        }
                        markers.append({
                            "time": middle_candle_date.strftime('%Y-%m-%d'),
                            "position": "aboveBar",
                            "color": color_map.get(fvg.get('status'), '#FFD700'),
                            "shape": "circle",
                            "text": "ğŸ®"
                        })
            except Exception as e:
                logger.warning(f"FVGãƒãƒ¼ã‚«ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {symbol_data.get('symbol', 'N/A')} - {e}")

        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ¼ã‚«ãƒ¼ï¼ˆãƒã‚¼ãƒ³ã‚¿ã§"Break"ï¼‰
        for s in symbol_data.get('signals', []):
            markers.append({
                "time": s['breakout_date'],
                "position": "belowBar",
                "color": "#FF00FF",  # ãƒã‚¼ãƒ³ã‚¿
                "shape": "arrowUp",
                "text": "Break"
            })

        return {
            'candles': candles,
            'sma200': format_series(df_plot, 'sma200'),
            'ema200': format_series(df_plot, 'ema200'),
            'weekly_sma200': format_series(df_plot, 'weekly_sma200_val'),
            'volume': [clean_np_types(v) for v in volume_data],
            'markers': [clean_np_types(m) for m in markers]
        }

    def _create_daily_summary(self, results: List[Dict], total_scanned: int, start_time: datetime) -> Dict:
        """æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        end_time = datetime.now()
        signals = sorted([r for r in results if r['signal_type'] == 'signal'], key=lambda x: x['score'], reverse=True)
        candidates = sorted([r for r in results if r['signal_type'] == 'candidate'], key=lambda x: x['score'], reverse=True)
        
        return {
            "scan_date": end_time.strftime('%Y-%m-%d'),
            "scan_time": end_time.strftime('%H:%M:%S'),
            "scan_duration_seconds": (end_time - start_time).total_seconds(),
            "total_scanned": total_scanned,
            "summary": {
                "signals_count": len(signals),
                "candidates_count": len(candidates),
                "signals": signals,
                "candidates": candidates
            },
            "performance": {
                "avg_time_per_symbol_ms": ((end_time - start_time).total_seconds() / total_scanned * 1000) if total_scanned > 0 else 0
            }
        }


async def run_hwb_scan(progress_callback=None):
    """ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    scanner = HWBScanner()
    summary = await scanner.scan_all_symbols(progress_callback)
    logger.info(f"å®Œäº† - ã‚·ã‚°ãƒŠãƒ«: {summary['summary']['signals_count']}, å€™è£œ: {summary['summary']['candidates_count']}")
    return summary


async def analyze_single_ticker(symbol: str) -> Optional[Dict]:
    """å˜ä¸€éŠ˜æŸ„åˆ†æ"""
    scanner = HWBScanner()
    scanner._analyze_and_save_symbol(symbol)
    return scanner.data_manager.load_symbol_data(symbol)